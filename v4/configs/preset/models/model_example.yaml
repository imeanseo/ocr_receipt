# @package _global_

defaults:
  - /preset/models/decoder/unet
  - /preset/models/encoder/timm_backbone
  - /preset/models/head/db_head
  - /preset/models/loss/db_loss
  - _self_

models:
  optimizer:
    _target_: torch.optim.AdamW # AdamW로 변경
    lr: 0.001
    weight_decay: 0.0001
  scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts # 코사인 스케쥴러로 변경
    T_0: 20        # 첫 주기 길이
    T_mult: 2      # 주기 증가 배율
    eta_min: 1e-6
  # scheduler:
  #   _target_: torch.optim.lr_scheduler.StepLR
  #   step_size: 100
  #   gamma: 0.1
